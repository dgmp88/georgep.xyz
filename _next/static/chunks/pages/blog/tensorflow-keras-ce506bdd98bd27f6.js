(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[419],{1498:function(e,t,n){(window.__NEXT_P=window.__NEXT_P||[]).push(["/blog/tensorflow-keras",function(){return n(4682)}])},8492:function(e,t,n){"use strict";n.d(t,{h:function(){return o}});var r=n(5893),a=n(1664);function o(){return(0,r.jsxs)("div",{className:"navbar",children:[(0,r.jsx)("div",{className:"flex-1",children:(0,r.jsx)(a.default,{href:"/",children:(0,r.jsx)("a",{className:"btn btn-ghost normal-case text-xl",children:"George Prichard"})})}),(0,r.jsx)("div",{className:"flex-none",children:(0,r.jsx)("ul",{className:"menu menu-horizontal p-0",children:(0,r.jsx)("li",{children:(0,r.jsx)(a.default,{href:"/blog",children:"Blog"})})})})]})}},8894:function(e,t,n){"use strict";n.d(t,{Z:function(){return u}});var r=n(5893),a=(n(7294),n(9008)),o=n(3905),s=n(8492);function l(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var i={};function u(e){var t=e.meta,n=e.route,u=(l(e,["meta","route"]),(0,r.jsxs)(a.default,{children:[(0,r.jsx)("meta",{property:"og:title",content:t.title}),(0,r.jsx)("meta",{property:"og:site_name",content:"George Prichard's blog"}),(0,r.jsx)("meta",{property:"og:description",content:t.description}),(0,r.jsx)("meta",{property:"og:image",content:t.og})]}));return n.startsWith("/blog")?function(e){var n=e.children;return(0,r.jsxs)(r.Fragment,{children:[u,(0,r.jsx)(s.h,{}),(0,r.jsxs)("div",{className:"prose lg:prose-lg mx-auto px-4 py-8",children:[(0,r.jsx)("h2",{children:t.title}),(0,r.jsx)(o.Zo,{components:i,children:n})]})]})}:function(e){var t=e.children;return(0,r.jsxs)(r.Fragment,{children:[u,t]})}}},4682:function(e,t,n){"use strict";n.r(t);n(7294);var r=n(3905),a=n(8894),o=n(3805);function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=function(e){return(0,o.withSSG)((0,a.Z)({filename:"tensorflow-keras.mdx",route:"/blog/tensorflow-keras",meta:{title:"Tensorflow vs Keras",date:"2017/01/02"},pageMap:[{name:"blog",children:[{name:"4k-weeks",route:"/blog/4k-weeks",frontMatter:{title:"4,000 Weeks",date:"2022/04/03"}},{name:"expected-value",route:"/blog/expected-value",frontMatter:{title:"What Expected Value Misses",date:"2022/04/17"}},{name:"lunee",route:"/blog/lunee",frontMatter:{title:"Lunee",date:"2022/04/13"}},{name:"prorogation",route:"/blog/prorogation",frontMatter:{title:"How unusual is this prorogation",date:"2019/09/16"}},{name:"python-plotting",route:"/blog/python-plotting",frontMatter:{title:"Python Plotting Libraries",date:"2019/01/20"}},{name:"python-protobuf-import-error",route:"/blog/python-protobuf-import-error",frontMatter:{title:"Python3 Protobuf Import Error",date:"2017/01/02"}},{name:"tensorflow-keras",route:"/blog/tensorflow-keras",frontMatter:{title:"Tensorflow vs Keras",date:"2017/01/02"}}],route:"/blog"}]},null))(e)};function i(e){var t=e.components,n=s(e,["components"]);return(0,r.kt)(l,Object.assign({components:t},n),(0,r.kt)("h1",null,"Tensorflow vs Keras"),(0,r.kt)("p",null,"TLDR: if you want to use deep learning as a tool, you should probably use Keras. If you're researching deep learning itself, you might need the fine grained control of TensorFlow."),(0,r.kt)("p",null,"Recently I\u2019ve had some ideas for deep learning projects, and wanted to use Python to do it because it\u2019s great for rapid prototyping and I\u2019m comfortable with some great relevant libraries like numpy, pandas and Pillow. I like Torch7, but am just not as confident in Lua and would miss those tools. TensorFlow has a lot of momentum, but has a rather high barrier to entry. What Keras does is provide a high-level interface on top of TensorFlow. It also lets you use Theano as a back end, but we wont go in to that here."),(0,r.kt)("p",null,"I\u2019m comparing Tensorflow against ",(0,r.kt)("a",{href:"http://keras.io/",parentName:"p"},"Keras")," instead of similar projects like ",(0,r.kt)("a",{href:"https://github.com/tflearn/tflearn",parentName:"p"},"TFLearn"),", ",(0,r.kt)("a",{href:"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim",parentName:"p"},"TFSlim"),", or another ",(0,r.kt)("a",{href:"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn",parentName:"p"},"TFLearn"),", mainly because Keras seems to be the most established in terms of buzz I\u2019ve heard online, the amount of work done on each of those Github repos, and it felt like a general Keras-consensus on ",(0,r.kt)("a",{href:"https://www.reddit.com/r/MachineLearning/comments/50eokb/which_one_should_i_choose_keras_tensorlayer/",parentName:"p"},"this")," Reddit thread. NB: that thread suggests Keras may be somewhat slower than pure Tensorflow. However, for the sake of my projects rapid prototyping is initially more valuable than rapid training. Lets see if I live to regret that statement."),(0,r.kt)("p",null,"All the code is available in ",(0,r.kt)("a",{href:"https://github.com/dgmp88/nn-playground/blob/master/frameworks/TensorFlow%20vs%20Keras.ipynb",parentName:"p"},"this")," Jupyter notebook."),(0,r.kt)("h2",null,"The Task"),(0,r.kt)("p",null,"I\u2019d like to experiment with a few methods for using neural networks for sorting arrays. I\u2019m curious how they can do, and it\u2019s going to be crazy easy to get loads of training data. We'll use a dead simple 1 layer network (OK, this isn't really deep learning), and array of length 2 which should be pretty easy to sort."),(0,r.kt)("p",null,"Here\u2019s how we get data in numpy (after some boring imports):"),(0,r.kt)("pre",null,(0,r.kt)("code",{className:"language-python",parentName:"pre"},"import seaborn as sns\n\ndef get_batch(array_size, n_samples=100):\n    x = np.random.rand(n_samples, array_size)\n    y = x.argsort()\n    return x, y\n")),(0,r.kt)("p",null,"I\u2019m choosing to try and get the network to predict the results of an argsort here: the indices needed to sort the initial array. My thinking here is I can just then round the end result and use them for sorting my array, and end up with identical outputs instead of the network being penalised for not reproducing the inputs ",(0,r.kt)("em",{parentName:"p"},"exactly")," in order. This certainly bears some testing, but not for the sake of this post. It also enables a neat evaluation function, which you can interpret as 'percent of order integers that were correct':"),(0,r.kt)("pre",null,(0,r.kt)("code",{className:"language-python",parentName:"pre"},"def evaluate(y_pred, y_actual):\n    score = np.mean(y_pred.round() == y_actual)*100\n    return score\n")),(0,r.kt)("p",null,"Before we get any further, lets also define some hyperparameters we'll use in all the tests."),(0,r.kt)("pre",null,(0,r.kt)("code",{className:"language-python",parentName:"pre"},"array_size = 2 # How long the arrays are we'll sort. Not very long at all.\nruns = 500 # Number of training runs\nbatch_size=10000 # Number of training examples in each training run\n")),(0,r.kt)("h2",null,"Code"),(0,r.kt)("h3",null,"Defining the models"),(0,r.kt)("pre",null,(0,r.kt)("code",{className:"language-python",parentName:"pre"},"\nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\n# Placeholder variables for the inputs and outputs\nx = tf.placeholder(tf.float32, shape=[None, array_size])\ny_ = tf.placeholder(tf.float32, shape=[None, array_size])\n\n# Set up the variables for our fully connected layer\nW = tf.Variable(tf.zeros([array_size, array_size]))\nb = tf.Variable(tf.zeros([array_size]))\n\n# Implement our fully connected layer\ny = tf.matmul(x,W) + b\n\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{className:"language-python",parentName:"pre"},"\nimport keras\n\n# Initiate our model\n\nmodel = keras.models.Sequential()\n\n# Add the layers\n\nlayer = keras.layers.Dense(\noutput_dim=array_size,\ninput_dim=array_size)\nmodel.add(layer)\n\n")),(0,r.kt)("h4",null,"Tensorflow"),(0,r.kt)("pre",null,(0,r.kt)("code",{className:"language-python",parentName:"pre"},"import tensorflow as tf\nsess = tf.InteractiveSession()\n\n# Placeholder variables for the inputs and outputs\n\nshape = [None, array_size]\nx = tf.placeholder(tf.float32, shape=shape)\ny\\_ = tf.placeholder(tf.float32, shape=shape)\n\n# Set up the variables for our fully connected layer\n\nW = tf.Variable(tf.zeros([array_size, array_size]))\nb = tf.Variable(tf.zeros([array_size]))\n\n# Implement our fully connected layer\n\ny = tf.matmul(x,W) + b\n\n")),(0,r.kt)("h4",null,"Keras"),(0,r.kt)("pre",null,(0,r.kt)("code",{className:"language-python",parentName:"pre"},"import keras\n\n# Initiate our model\nmodel = keras.models.Sequential()\n\n# Add the layers\nlayer = keras.layers.Dense(\n    output_dim=array_size,\n    input_dim=array_size)\nmodel.add(layer)\n")),(0,r.kt)("h4",null,"Tensorflow"),(0,r.kt)("pre",null,(0,r.kt)("code",{className:"language-python",parentName:"pre"},"import tensorflow as tf\nsess = tf.InteractiveSession()\n\n# Placeholder variables for the inputs and outputs\nx = tf.placeholder(tf.float32, shape=[None, array_size])\ny_ = tf.placeholder(tf.float32, shape=[None, array_size])\n\n# Set up the variables for our fully connected layer\nW = tf.Variable(tf.zeros([array_size, array_size]))\nb = tf.Variable(tf.zeros([array_size]))\n\n# Implement our fully connected layer\ny = tf.matmul(x,W) + b\n")),(0,r.kt)("h4",null,"Keras"),(0,r.kt)("pre",null,(0,r.kt)("code",{className:"language-python",parentName:"pre"},"import keras\n\n# Initiate our model\nmodel = keras.models.Sequential()\n\n# Add the layers\nlayer = keras.layers.Dense(output_dim=array_size, input_dim=array_size)\nmodel.add(layer)\n")),(0,r.kt)("p",null,"So:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"TensorFlow has the concept of a 'session', where our variables are instantiated, separate from our model."),(0,r.kt)("li",{parentName:"ul"},"TensorFlow requires you to basically write out the maths behind a fully connected layer.")),(0,r.kt)("p",null,"For me, I've pretty much found a winner at this point. The level of abstraction Keras offers is far more pleasant and quick to work with."),(0,r.kt)("h3",null,"Defining the loss and optimisation functions"),(0,r.kt)("h4",null,"Tensorflow"),(0,r.kt)("pre",null,(0,r.kt)("code",{className:"language-python",parentName:"pre"},"\n# We're going to use mean squared error\nerror = tf.reduce_mean(tf.square(tf.sub(y_, y)))\n\n# Our optimizer\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(error)\n\n# Initialize all our variables\nsess.run(tf.global_variables_initializer())\n\n")),(0,r.kt)("h4",null,"Keras"),(0,r.kt)("pre",null,(0,r.kt)("code",{className:"language-python",parentName:"pre"},"\nsgd = keras.optimizers.SGD(lr=0.5)\nmodel.compile(loss='mse', optimizer=sgd)\n\n")),(0,r.kt)("p",null,"In Tensorflow, again, we have to write out all the maths for mean squared error using their functions. Again, I found this much easier in Keras."),(0,r.kt)("p",null,"We also initialize both models here - model.compile does this in Keras behind the scenes."),(0,r.kt)("h3",null,"Training"),(0,r.kt)("h4",null,"Tensorflow"),(0,r.kt)("pre",null,(0,r.kt)("code",{className:"language-python",parentName:"pre"},"\nlosses = []\nfor i in range(runs):\n    x_train, y_train = get_batch(array_size, batch_size)\n    _, loss_val = sess.run([train_step, error],\n                           feed_dict={x: x_train, y_:y_train})\n    losses.append(loss_val)\n\n")),(0,r.kt)("h4",null,"Keras"),(0,r.kt)("pre",null,(0,r.kt)("code",{className:"language-python",parentName:"pre"},"\nlosses = []\nfor i in range(runs):\n    x_train, y_train = get_batch(array_size, batch_size)\n    loss = model.train_on_batch(x_train, y_train)\n    losses.append(loss)\n\n")),(0,r.kt)("p",null,"The code length here is about the same. Things to note with TF: you run training in the session, and have to define 'fetches' - the graph elements you want to evaluate and return - as the first argument."),(0,r.kt)("h3",null,"Code conclusion"),(0,r.kt)("p",null,"For me this a hands-down win for Keras. Most people writing networks just wont need the control TensorFlow gives you, and the level of control comes at the high price of not giving you tools you'll want out of the box (fully connected layers, various error functions)."),(0,r.kt)("h3",null,"Aside validation"),(0,r.kt)("p",null,"To boost my confidence they're doing the same thing, I plotted the learning rate over time of both with a large batch size."),(0,r.kt)("p",null,(0,r.kt)("img",{src:"/images/tf_keras_lr.png",alt:"Learning rates",parentName:"p"})),(0,r.kt)("p",null,"The initially faster learning for TensorFlow was consistent, but they end up in exactly the same place every time. They also scored very similarly on the evaluation function (both above 99% on several runs)."),(0,r.kt)("h2",null,"Speed"),(0,r.kt)("p",null,"While I still maintain this is secondary, it's easy and interesting to do a benchmark. This isn't a great benchmark, as the network is so simple, but I'm curious to have an indication. So, the time it takes each to train each using ipythons %timeit:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"TensorFlow: 1.42s"),(0,r.kt)("li",{parentName:"ul"},"Keras: 1.45s")),(0,r.kt)("p",null,"Pretty neck and neck. I ran it a few times and the results varied a lot, but I don't think there's anything in it. However, ",(0,r.kt)("a",{href:"https://github.com/wagamamaz/tensorflow-wrapper-compare",parentName:"p"},"this")," found TensorFlow much (2x) faster and looks like a more real world test - I'd be slightly inclined to trust that more. But 2x isn't ",(0,r.kt)("em",{parentName:"p"},"too")," bad for me."),(0,r.kt)("h2",null,"Conclusion"),(0,r.kt)("p",null,"I'll start learning Keras properly - the API was way faster and easier to develop with for this test case. Check out the code in ",(0,r.kt)("a",{href:"https://github.com/dgmp88/nn-playground/blob/master/frameworks/TensorFlow%20vs%20Keras.ipynb",parentName:"p"},"notebook"),", looking at both all the way through makes you appreciate the simplicity even more."),(0,r.kt)("p",null,"Comment below if you have any thoughts, questions, or strong opinions on DL frameworks!"))}i.isMDXComponent=!0,t.default=i}},function(e){e.O(0,[977,774,888,179],(function(){return t=1498,e(e.s=t);var t}));var t=e.O();_N_E=t}]);