<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/3cdde1e1bec55b28.css" as="style"/><link rel="stylesheet" href="/_next/static/css/3cdde1e1bec55b28.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-e7ac09da019d6177.js" defer=""></script><script src="/_next/static/chunks/framework-fc97f3f1282ce3ed.js" defer=""></script><script src="/_next/static/chunks/main-f4ae3437c92c1efc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d5c71a350085f9f1.js" defer=""></script><script src="/_next/static/chunks/586-fc70be302fad1b1f.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-cceaa0e60c066113.js" defer=""></script><script src="/_next/static/UNz-08awTgYk1k5H4jdDr/_buildManifest.js" defer=""></script><script src="/_next/static/UNz-08awTgYk1k5H4jdDr/_ssgManifest.js" defer=""></script><script src="/_next/static/UNz-08awTgYk1k5H4jdDr/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div><div class="fixed w-screen h-screen" style="z-index:-1;background-image:url(&#x27;/curves.svg&#x27;);background-repeat:no-repeat;background-size:cover"></div></div><div class="absolute navbar text-gray-100 "><div class="flex-1"><a class="btn btn-ghost normal-case text-xl" href="/">George Prichard</a></div><div class="flex-none"><ul class="menu menu-horizontal p-0"><li><a href="/backgrounds">Backgrounds</a><a href="/blog">Blog</a></li></ul></div></div><div class="w-2/3 m-auto py-20"><div class="prose lg:prose-lg mx-auto px-4 py-8 bg-base-100/75 rounded-lg "><h1>Tensorflow vs Keras</h1><div><h1 id="tensorflowvskeras">Tensorflow vs Keras</h1>
<p>TLDR: if you want to use deep learning as a tool, you should probably use Keras. If you're researching deep learning itself, you might need the fine grained control of TensorFlow.</p>
<p>Recently I’ve had some ideas for deep learning projects, and wanted to use Python to do it because it’s great for rapid prototyping and I’m comfortable with some great relevant libraries like numpy, pandas and Pillow. I like Torch7, but am just not as confident in Lua and would miss those tools. TensorFlow has a lot of momentum, but has a rather high barrier to entry. What Keras does is provide a high-level interface on top of TensorFlow. It also lets you use Theano as a back end, but we wont go in to that here.</p>
<p>I’m comparing Tensorflow against <a href="http://keras.io/">Keras</a> instead of similar projects like <a href="https://github.com/tflearn/tflearn">TFLearn</a>, <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim">TFSlim</a>, or another <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn">TFLearn</a>, mainly because Keras seems to be the most established in terms of buzz I’ve heard online, the amount of work done on each of those Github repos, and it felt like a general Keras-consensus on <a href="https://www.reddit.com/r/MachineLearning/comments/50eokb/which_one_should_i_choose_keras_tensorlayer/">this</a> Reddit thread. NB: that thread suggests Keras may be somewhat slower than pure Tensorflow. However, for the sake of my projects rapid prototyping is initially more valuable than rapid training. Lets see if I live to regret that statement.</p>
<p>All the code is available in <a href="https://github.com/dgmp88/nn-playground/blob/master/frameworks/TensorFlow%20vs%20Keras.ipynb">this</a> Jupyter notebook.</p>
<h2 id="thetask">The Task</h2>
<p>I’d like to experiment with a few methods for using neural networks for sorting arrays. I’m curious how they can do, and it’s going to be crazy easy to get loads of training data. We'll use a dead simple 1 layer network (OK, this isn't really deep learning), and array of length 2 which should be pretty easy to sort.</p>
<p>Here’s how we get data in numpy (after some boring imports):</p>
<pre><code class="python language-python">import seaborn as sns

def get_batch(array_size, n_samples=100):
    x = np.random.rand(n_samples, array_size)
    y = x.argsort()
    return x, y
</code></pre>
<p>I’m choosing to try and get the network to predict the results of an argsort here: the indices needed to sort the initial array. My thinking here is I can just then round the end result and use them for sorting my array, and end up with identical outputs instead of the network being penalised for not reproducing the inputs <em>exactly</em> in order. This certainly bears some testing, but not for the sake of this post. It also enables a neat evaluation function, which you can interpret as 'percent of order integers that were correct':</p>
<pre><code class="python language-python">def evaluate(y_pred, y_actual):
    score = np.mean(y_pred.round() == y_actual)*100
    return score
</code></pre>
<p>Before we get any further, lets also define some hyperparameters we'll use in all the tests.</p>
<pre><code class="python language-python">array_size = 2 # How long the arrays are we'll sort. Not very long at all.
runs = 500 # Number of training runs
batch_size=10000 # Number of training examples in each training run
</code></pre>
<h2 id="code">Code</h2>
<h3 id="definingthemodels">Defining the models</h3>
<pre><code class="python language-python">import tensorflow as tf
sess = tf.InteractiveSession()

# Placeholder variables for the inputs and outputs
x = tf.placeholder(tf.float32, shape=[None, array_size])
y_ = tf.placeholder(tf.float32, shape=[None, array_size])

# Set up the variables for our fully connected layer
W = tf.Variable(tf.zeros([array_size, array_size]))
b = tf.Variable(tf.zeros([array_size]))

# Implement our fully connected layer
y = tf.matmul(x,W) + b
</code></pre>
<pre><code class="python language-python">import keras

# Initiate our model

model = keras.models.Sequential()

# Add the layers

layer = keras.layers.Dense(
output_dim=array_size,
input_dim=array_size)
model.add(layer)
</code></pre>
<h4 id="tensorflow">Tensorflow</h4>
<pre><code class="python language-python">import tensorflow as tf
sess = tf.InteractiveSession()

# Placeholder variables for the inputs and outputs

shape = [None, array_size]
x = tf.placeholder(tf.float32, shape=shape)
y\_ = tf.placeholder(tf.float32, shape=shape)

# Set up the variables for our fully connected layer

W = tf.Variable(tf.zeros([array_size, array_size]))
b = tf.Variable(tf.zeros([array_size]))

# Implement our fully connected layer

y = tf.matmul(x,W) + b
</code></pre>
<h4 id="keras">Keras</h4>
<pre><code class="python language-python">import keras

# Initiate our model
model = keras.models.Sequential()

# Add the layers
layer = keras.layers.Dense(
    output_dim=array_size,
    input_dim=array_size)
model.add(layer)
</code></pre>
<h4 id="tensorflow-1">Tensorflow</h4>
<pre><code class="python language-python">import tensorflow as tf
sess = tf.InteractiveSession()

# Placeholder variables for the inputs and outputs
x = tf.placeholder(tf.float32, shape=[None, array_size])
y_ = tf.placeholder(tf.float32, shape=[None, array_size])

# Set up the variables for our fully connected layer
W = tf.Variable(tf.zeros([array_size, array_size]))
b = tf.Variable(tf.zeros([array_size]))

# Implement our fully connected layer
y = tf.matmul(x,W) + b
</code></pre>
<h4 id="keras-1">Keras</h4>
<pre><code class="python language-python">import keras

# Initiate our model
model = keras.models.Sequential()

# Add the layers
layer = keras.layers.Dense(output_dim=array_size, input_dim=array_size)
model.add(layer)
</code></pre>
<p>So:</p>
<ul>
<li>TensorFlow has the concept of a 'session', where our variables are instantiated, separate from our model.</li>
<li>TensorFlow requires you to basically write out the maths behind a fully connected layer.</li>
</ul>
<p>For me, I've pretty much found a winner at this point. The level of abstraction Keras offers is far more pleasant and quick to work with.</p>
<h3 id="definingthelossandoptimisationfunctions">Defining the loss and optimisation functions</h3>
<h4 id="tensorflow-2">Tensorflow</h4>
<pre><code class="python language-python"># We're going to use mean squared error
error = tf.reduce_mean(tf.square(tf.sub(y_, y)))

# Our optimizer
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(error)

# Initialize all our variables
sess.run(tf.global_variables_initializer())
</code></pre>
<h4 id="keras-2">Keras</h4>
<pre><code class="python language-python">sgd = keras.optimizers.SGD(lr=0.5)
model.compile(loss='mse', optimizer=sgd)
</code></pre>
<p>In Tensorflow, again, we have to write out all the maths for mean squared error using their functions. Again, I found this much easier in Keras.</p>
<p>We also initialize both models here - model.compile does this in Keras behind the scenes.</p>
<h3 id="training">Training</h3>
<h4 id="tensorflow-3">Tensorflow</h4>
<pre><code class="python language-python">losses = []
for i in range(runs):
    x_train, y_train = get_batch(array_size, batch_size)
    _, loss_val = sess.run([train_step, error],
                           feed_dict={x: x_train, y_:y_train})
    losses.append(loss_val)
</code></pre>
<h4 id="keras-3">Keras</h4>
<pre><code class="python language-python">losses = []
for i in range(runs):
    x_train, y_train = get_batch(array_size, batch_size)
    loss = model.train_on_batch(x_train, y_train)
    losses.append(loss)
</code></pre>
<p>The code length here is about the same. Things to note with TF: you run training in the session, and have to define 'fetches' - the graph elements you want to evaluate and return - as the first argument.</p>
<h3 id="codeconclusion">Code conclusion</h3>
<p>For me this a hands-down win for Keras. Most people writing networks just wont need the control TensorFlow gives you, and the level of control comes at the high price of not giving you tools you'll want out of the box (fully connected layers, various error functions).</p>
<h3 id="asidevalidation">Aside validation</h3>
<p>To boost my confidence they're doing the same thing, I plotted the learning rate over time of both with a large batch size.</p>
<p><img src="/images/tf_keras_lr.png" alt="Learning rates" /></p>
<p>The initially faster learning for TensorFlow was consistent, but they end up in exactly the same place every time. They also scored very similarly on the evaluation function (both above 99% on several runs).</p>
<h2 id="speed">Speed</h2>
<p>While I still maintain this is secondary, it's easy and interesting to do a benchmark. This isn't a great benchmark, as the network is so simple, but I'm curious to have an indication. So, the time it takes each to train each using ipythons %timeit:</p>
<ul>
<li>TensorFlow: 1.42s</li>
<li>Keras: 1.45s</li>
</ul>
<p>Pretty neck and neck. I ran it a few times and the results varied a lot, but I don't think there's anything in it. However, <a href="https://github.com/wagamamaz/tensorflow-wrapper-compare">this</a> found TensorFlow much (2x) faster and looks like a more real world test - I'd be slightly inclined to trust that more. But 2x isn't <em>too</em> bad for me.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I'll start learning Keras properly - the API was way faster and easier to develop with for this test case. Check out the code in <a href="https://github.com/dgmp88/nn-playground/blob/master/frameworks/TensorFlow%20vs%20Keras.ipynb">notebook</a>, looking at both all the way through makes you appreciate the simplicity even more.</p>
<p>Comment below if you have any thoughts, questions, or strong opinions on DL frameworks!</p></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"meta":{"title":"Tensorflow vs Keras","date":"2017/01/02"},"content":"\u003ch1 id=\"tensorflowvskeras\"\u003eTensorflow vs Keras\u003c/h1\u003e\n\u003cp\u003eTLDR: if you want to use deep learning as a tool, you should probably use Keras. If you're researching deep learning itself, you might need the fine grained control of TensorFlow.\u003c/p\u003e\n\u003cp\u003eRecently I’ve had some ideas for deep learning projects, and wanted to use Python to do it because it’s great for rapid prototyping and I’m comfortable with some great relevant libraries like numpy, pandas and Pillow. I like Torch7, but am just not as confident in Lua and would miss those tools. TensorFlow has a lot of momentum, but has a rather high barrier to entry. What Keras does is provide a high-level interface on top of TensorFlow. It also lets you use Theano as a back end, but we wont go in to that here.\u003c/p\u003e\n\u003cp\u003eI’m comparing Tensorflow against \u003ca href=\"http://keras.io/\"\u003eKeras\u003c/a\u003e instead of similar projects like \u003ca href=\"https://github.com/tflearn/tflearn\"\u003eTFLearn\u003c/a\u003e, \u003ca href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim\"\u003eTFSlim\u003c/a\u003e, or another \u003ca href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn\"\u003eTFLearn\u003c/a\u003e, mainly because Keras seems to be the most established in terms of buzz I’ve heard online, the amount of work done on each of those Github repos, and it felt like a general Keras-consensus on \u003ca href=\"https://www.reddit.com/r/MachineLearning/comments/50eokb/which_one_should_i_choose_keras_tensorlayer/\"\u003ethis\u003c/a\u003e Reddit thread. NB: that thread suggests Keras may be somewhat slower than pure Tensorflow. However, for the sake of my projects rapid prototyping is initially more valuable than rapid training. Lets see if I live to regret that statement.\u003c/p\u003e\n\u003cp\u003eAll the code is available in \u003ca href=\"https://github.com/dgmp88/nn-playground/blob/master/frameworks/TensorFlow%20vs%20Keras.ipynb\"\u003ethis\u003c/a\u003e Jupyter notebook.\u003c/p\u003e\n\u003ch2 id=\"thetask\"\u003eThe Task\u003c/h2\u003e\n\u003cp\u003eI’d like to experiment with a few methods for using neural networks for sorting arrays. I’m curious how they can do, and it’s going to be crazy easy to get loads of training data. We'll use a dead simple 1 layer network (OK, this isn't really deep learning), and array of length 2 which should be pretty easy to sort.\u003c/p\u003e\n\u003cp\u003eHere’s how we get data in numpy (after some boring imports):\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python language-python\"\u003eimport seaborn as sns\n\ndef get_batch(array_size, n_samples=100):\n    x = np.random.rand(n_samples, array_size)\n    y = x.argsort()\n    return x, y\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eI’m choosing to try and get the network to predict the results of an argsort here: the indices needed to sort the initial array. My thinking here is I can just then round the end result and use them for sorting my array, and end up with identical outputs instead of the network being penalised for not reproducing the inputs \u003cem\u003eexactly\u003c/em\u003e in order. This certainly bears some testing, but not for the sake of this post. It also enables a neat evaluation function, which you can interpret as 'percent of order integers that were correct':\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python language-python\"\u003edef evaluate(y_pred, y_actual):\n    score = np.mean(y_pred.round() == y_actual)*100\n    return score\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBefore we get any further, lets also define some hyperparameters we'll use in all the tests.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python language-python\"\u003earray_size = 2 # How long the arrays are we'll sort. Not very long at all.\nruns = 500 # Number of training runs\nbatch_size=10000 # Number of training examples in each training run\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"code\"\u003eCode\u003c/h2\u003e\n\u003ch3 id=\"definingthemodels\"\u003eDefining the models\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"python language-python\"\u003eimport tensorflow as tf\nsess = tf.InteractiveSession()\n\n# Placeholder variables for the inputs and outputs\nx = tf.placeholder(tf.float32, shape=[None, array_size])\ny_ = tf.placeholder(tf.float32, shape=[None, array_size])\n\n# Set up the variables for our fully connected layer\nW = tf.Variable(tf.zeros([array_size, array_size]))\nb = tf.Variable(tf.zeros([array_size]))\n\n# Implement our fully connected layer\ny = tf.matmul(x,W) + b\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"python language-python\"\u003eimport keras\n\n# Initiate our model\n\nmodel = keras.models.Sequential()\n\n# Add the layers\n\nlayer = keras.layers.Dense(\noutput_dim=array_size,\ninput_dim=array_size)\nmodel.add(layer)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"tensorflow\"\u003eTensorflow\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"python language-python\"\u003eimport tensorflow as tf\nsess = tf.InteractiveSession()\n\n# Placeholder variables for the inputs and outputs\n\nshape = [None, array_size]\nx = tf.placeholder(tf.float32, shape=shape)\ny\\_ = tf.placeholder(tf.float32, shape=shape)\n\n# Set up the variables for our fully connected layer\n\nW = tf.Variable(tf.zeros([array_size, array_size]))\nb = tf.Variable(tf.zeros([array_size]))\n\n# Implement our fully connected layer\n\ny = tf.matmul(x,W) + b\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"keras\"\u003eKeras\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"python language-python\"\u003eimport keras\n\n# Initiate our model\nmodel = keras.models.Sequential()\n\n# Add the layers\nlayer = keras.layers.Dense(\n    output_dim=array_size,\n    input_dim=array_size)\nmodel.add(layer)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"tensorflow-1\"\u003eTensorflow\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"python language-python\"\u003eimport tensorflow as tf\nsess = tf.InteractiveSession()\n\n# Placeholder variables for the inputs and outputs\nx = tf.placeholder(tf.float32, shape=[None, array_size])\ny_ = tf.placeholder(tf.float32, shape=[None, array_size])\n\n# Set up the variables for our fully connected layer\nW = tf.Variable(tf.zeros([array_size, array_size]))\nb = tf.Variable(tf.zeros([array_size]))\n\n# Implement our fully connected layer\ny = tf.matmul(x,W) + b\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"keras-1\"\u003eKeras\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"python language-python\"\u003eimport keras\n\n# Initiate our model\nmodel = keras.models.Sequential()\n\n# Add the layers\nlayer = keras.layers.Dense(output_dim=array_size, input_dim=array_size)\nmodel.add(layer)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSo:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTensorFlow has the concept of a 'session', where our variables are instantiated, separate from our model.\u003c/li\u003e\n\u003cli\u003eTensorFlow requires you to basically write out the maths behind a fully connected layer.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor me, I've pretty much found a winner at this point. The level of abstraction Keras offers is far more pleasant and quick to work with.\u003c/p\u003e\n\u003ch3 id=\"definingthelossandoptimisationfunctions\"\u003eDefining the loss and optimisation functions\u003c/h3\u003e\n\u003ch4 id=\"tensorflow-2\"\u003eTensorflow\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"python language-python\"\u003e# We're going to use mean squared error\nerror = tf.reduce_mean(tf.square(tf.sub(y_, y)))\n\n# Our optimizer\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(error)\n\n# Initialize all our variables\nsess.run(tf.global_variables_initializer())\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"keras-2\"\u003eKeras\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"python language-python\"\u003esgd = keras.optimizers.SGD(lr=0.5)\nmodel.compile(loss='mse', optimizer=sgd)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn Tensorflow, again, we have to write out all the maths for mean squared error using their functions. Again, I found this much easier in Keras.\u003c/p\u003e\n\u003cp\u003eWe also initialize both models here - model.compile does this in Keras behind the scenes.\u003c/p\u003e\n\u003ch3 id=\"training\"\u003eTraining\u003c/h3\u003e\n\u003ch4 id=\"tensorflow-3\"\u003eTensorflow\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"python language-python\"\u003elosses = []\nfor i in range(runs):\n    x_train, y_train = get_batch(array_size, batch_size)\n    _, loss_val = sess.run([train_step, error],\n                           feed_dict={x: x_train, y_:y_train})\n    losses.append(loss_val)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"keras-3\"\u003eKeras\u003c/h4\u003e\n\u003cpre\u003e\u003ccode class=\"python language-python\"\u003elosses = []\nfor i in range(runs):\n    x_train, y_train = get_batch(array_size, batch_size)\n    loss = model.train_on_batch(x_train, y_train)\n    losses.append(loss)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe code length here is about the same. Things to note with TF: you run training in the session, and have to define 'fetches' - the graph elements you want to evaluate and return - as the first argument.\u003c/p\u003e\n\u003ch3 id=\"codeconclusion\"\u003eCode conclusion\u003c/h3\u003e\n\u003cp\u003eFor me this a hands-down win for Keras. Most people writing networks just wont need the control TensorFlow gives you, and the level of control comes at the high price of not giving you tools you'll want out of the box (fully connected layers, various error functions).\u003c/p\u003e\n\u003ch3 id=\"asidevalidation\"\u003eAside validation\u003c/h3\u003e\n\u003cp\u003eTo boost my confidence they're doing the same thing, I plotted the learning rate over time of both with a large batch size.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/tf_keras_lr.png\" alt=\"Learning rates\" /\u003e\u003c/p\u003e\n\u003cp\u003eThe initially faster learning for TensorFlow was consistent, but they end up in exactly the same place every time. They also scored very similarly on the evaluation function (both above 99% on several runs).\u003c/p\u003e\n\u003ch2 id=\"speed\"\u003eSpeed\u003c/h2\u003e\n\u003cp\u003eWhile I still maintain this is secondary, it's easy and interesting to do a benchmark. This isn't a great benchmark, as the network is so simple, but I'm curious to have an indication. So, the time it takes each to train each using ipythons %timeit:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTensorFlow: 1.42s\u003c/li\u003e\n\u003cli\u003eKeras: 1.45s\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePretty neck and neck. I ran it a few times and the results varied a lot, but I don't think there's anything in it. However, \u003ca href=\"https://github.com/wagamamaz/tensorflow-wrapper-compare\"\u003ethis\u003c/a\u003e found TensorFlow much (2x) faster and looks like a more real world test - I'd be slightly inclined to trust that more. But 2x isn't \u003cem\u003etoo\u003c/em\u003e bad for me.\u003c/p\u003e\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eI'll start learning Keras properly - the API was way faster and easier to develop with for this test case. Check out the code in \u003ca href=\"https://github.com/dgmp88/nn-playground/blob/master/frameworks/TensorFlow%20vs%20Keras.ipynb\"\u003enotebook\u003c/a\u003e, looking at both all the way through makes you appreciate the simplicity even more.\u003c/p\u003e\n\u003cp\u003eComment below if you have any thoughts, questions, or strong opinions on DL frameworks!\u003c/p\u003e"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"tensorflow-keras"},"buildId":"UNz-08awTgYk1k5H4jdDr","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>