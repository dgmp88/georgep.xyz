<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><meta property="og:title" content="Tensorflow vs Keras"/><meta property="og:site_name" content="George Prichard&#x27;s blog"/><meta property="og:description"/><meta property="og:image"/><meta name="next-head-count" content="6"/><link rel="preload" href="/_next/static/css/568115390c07c336.css" as="style"/><link rel="stylesheet" href="/_next/static/css/568115390c07c336.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-d7b038a63b619762.js" defer=""></script><script src="/_next/static/chunks/framework-5f4595e5518b5600.js" defer=""></script><script src="/_next/static/chunks/main-7c9e8fd8226b6573.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c91b8fdce06a16b2.js" defer=""></script><script src="/_next/static/chunks/977-d02d6b0b927a7604.js" defer=""></script><script src="/_next/static/chunks/pages/blog/tensorflow-keras-b50813883e35967e.js" defer=""></script><script src="/_next/static/ExTSNI7gSL9kUh4Eg_T7t/_buildManifest.js" defer=""></script><script src="/_next/static/ExTSNI7gSL9kUh4Eg_T7t/_ssgManifest.js" defer=""></script><script src="/_next/static/ExTSNI7gSL9kUh4Eg_T7t/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><div class="navbar"><div class="flex-1"><a class="btn btn-ghost normal-case text-xl" href="/">George Prichard</a></div><div class="flex-none"><ul class="menu menu-horizontal p-0"><li><a href="/blog">Blog</a></li></ul></div></div><div class="prose lg:prose-lg mx-auto px-4 py-8"><h1>Tensorflow vs Keras</h1><p>TLDR: if you want to use deep learning as a tool, you should probably use Keras. If you&#x27;re researching deep learning itself, you might need the fine grained control of TensorFlow.</p><p>Recently I’ve had some ideas for deep learning projects, and wanted to use Python to do it because it’s great for rapid prototyping and I’m comfortable with some great relevant libraries like numpy, pandas and Pillow. I like Torch7, but am just not as confident in Lua and would miss those tools. TensorFlow has a lot of momentum, but has a rather high barrier to entry. What Keras does is provide a high-level interface on top of TensorFlow. It also lets you use Theano as a back end, but we wont go in to that here.</p><p>I’m comparing Tensorflow against <a href="http://keras.io/" parentName="p">Keras</a> instead of similar projects like <a href="https://github.com/tflearn/tflearn" parentName="p">TFLearn</a>, <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim" parentName="p">TFSlim</a>, or another <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn" parentName="p">TFLearn</a>, mainly because Keras seems to be the most established in terms of buzz I’ve heard online, the amount of work done on each of those Github repos, and it felt like a general Keras-consensus on <a href="https://www.reddit.com/r/MachineLearning/comments/50eokb/which_one_should_i_choose_keras_tensorlayer/" parentName="p">this</a> Reddit thread. NB: that thread suggests Keras may be somewhat slower than pure Tensorflow. However, for the sake of my projects rapid prototyping is initially more valuable than rapid training. Lets see if I live to regret that statement.</p><p>All the code is available in <a href="https://github.com/dgmp88/nn-playground/blob/master/frameworks/TensorFlow%20vs%20Keras.ipynb" parentName="p">this</a> Jupyter notebook.</p><h2>The Task</h2><p>I’d like to experiment with a few methods for using neural networks for sorting arrays. I’m curious how they can do, and it’s going to be crazy easy to get loads of training data. We&#x27;ll use a dead simple 1 layer network (OK, this isn&#x27;t really deep learning), and array of length 2 which should be pretty easy to sort.</p><p>Here’s how we get data in numpy (after some boring imports):</p><pre><code class="language-python" parentName="pre">import seaborn as sns

def get_batch(array_size, n_samples=100):
    x = np.random.rand(n_samples, array_size)
    y = x.argsort()
    return x, y
</code></pre><p>I’m choosing to try and get the network to predict the results of an argsort here: the indices needed to sort the initial array. My thinking here is I can just then round the end result and use them for sorting my array, and end up with identical outputs instead of the network being penalised for not reproducing the inputs <em parentName="p">exactly</em> in order. This certainly bears some testing, but not for the sake of this post. It also enables a neat evaluation function, which you can interpret as &#x27;percent of order integers that were correct&#x27;:</p><pre><code class="language-python" parentName="pre">def evaluate(y_pred, y_actual):
    score = np.mean(y_pred.round() == y_actual)*100
    return score
</code></pre><p>Before we get any further, lets also define some hyperparameters we&#x27;ll use in all the tests.</p><pre><code class="language-python" parentName="pre">array_size = 2 # How long the arrays are we&#x27;ll sort. Not very long at all.
runs = 500 # Number of training runs
batch_size=10000 # Number of training examples in each training run
</code></pre><h2>Code</h2><h3>Defining the models</h3><pre><code class="language-python" parentName="pre">
import tensorflow as tf
sess = tf.InteractiveSession()

# Placeholder variables for the inputs and outputs
x = tf.placeholder(tf.float32, shape=[None, array_size])
y_ = tf.placeholder(tf.float32, shape=[None, array_size])

# Set up the variables for our fully connected layer
W = tf.Variable(tf.zeros([array_size, array_size]))
b = tf.Variable(tf.zeros([array_size]))

# Implement our fully connected layer
y = tf.matmul(x,W) + b

</code></pre><pre><code class="language-python" parentName="pre">
import keras

# Initiate our model

model = keras.models.Sequential()

# Add the layers

layer = keras.layers.Dense(
output_dim=array_size,
input_dim=array_size)
model.add(layer)

</code></pre><h4>Tensorflow</h4><pre><code class="language-python" parentName="pre">import tensorflow as tf
sess = tf.InteractiveSession()

# Placeholder variables for the inputs and outputs

shape = [None, array_size]
x = tf.placeholder(tf.float32, shape=shape)
y\_ = tf.placeholder(tf.float32, shape=shape)

# Set up the variables for our fully connected layer

W = tf.Variable(tf.zeros([array_size, array_size]))
b = tf.Variable(tf.zeros([array_size]))

# Implement our fully connected layer

y = tf.matmul(x,W) + b

</code></pre><h4>Keras</h4><pre><code class="language-python" parentName="pre">import keras

# Initiate our model
model = keras.models.Sequential()

# Add the layers
layer = keras.layers.Dense(
    output_dim=array_size,
    input_dim=array_size)
model.add(layer)
</code></pre><h4>Tensorflow</h4><pre><code class="language-python" parentName="pre">import tensorflow as tf
sess = tf.InteractiveSession()

# Placeholder variables for the inputs and outputs
x = tf.placeholder(tf.float32, shape=[None, array_size])
y_ = tf.placeholder(tf.float32, shape=[None, array_size])

# Set up the variables for our fully connected layer
W = tf.Variable(tf.zeros([array_size, array_size]))
b = tf.Variable(tf.zeros([array_size]))

# Implement our fully connected layer
y = tf.matmul(x,W) + b
</code></pre><h4>Keras</h4><pre><code class="language-python" parentName="pre">import keras

# Initiate our model
model = keras.models.Sequential()

# Add the layers
layer = keras.layers.Dense(output_dim=array_size, input_dim=array_size)
model.add(layer)
</code></pre><p>So:</p><ul><li parentName="ul">TensorFlow has the concept of a &#x27;session&#x27;, where our variables are instantiated, separate from our model.</li><li parentName="ul">TensorFlow requires you to basically write out the maths behind a fully connected layer.</li></ul><p>For me, I&#x27;ve pretty much found a winner at this point. The level of abstraction Keras offers is far more pleasant and quick to work with.</p><h3>Defining the loss and optimisation functions</h3><h4>Tensorflow</h4><pre><code class="language-python" parentName="pre">
# We&#x27;re going to use mean squared error
error = tf.reduce_mean(tf.square(tf.sub(y_, y)))

# Our optimizer
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(error)

# Initialize all our variables
sess.run(tf.global_variables_initializer())

</code></pre><h4>Keras</h4><pre><code class="language-python" parentName="pre">
sgd = keras.optimizers.SGD(lr=0.5)
model.compile(loss=&#x27;mse&#x27;, optimizer=sgd)

</code></pre><p>In Tensorflow, again, we have to write out all the maths for mean squared error using their functions. Again, I found this much easier in Keras.</p><p>We also initialize both models here - model.compile does this in Keras behind the scenes.</p><h3>Training</h3><h4>Tensorflow</h4><pre><code class="language-python" parentName="pre">
losses = []
for i in range(runs):
    x_train, y_train = get_batch(array_size, batch_size)
    _, loss_val = sess.run([train_step, error],
                           feed_dict={x: x_train, y_:y_train})
    losses.append(loss_val)

</code></pre><h4>Keras</h4><pre><code class="language-python" parentName="pre">
losses = []
for i in range(runs):
    x_train, y_train = get_batch(array_size, batch_size)
    loss = model.train_on_batch(x_train, y_train)
    losses.append(loss)

</code></pre><p>The code length here is about the same. Things to note with TF: you run training in the session, and have to define &#x27;fetches&#x27; - the graph elements you want to evaluate and return - as the first argument.</p><h3>Code conclusion</h3><p>For me this a hands-down win for Keras. Most people writing networks just wont need the control TensorFlow gives you, and the level of control comes at the high price of not giving you tools you&#x27;ll want out of the box (fully connected layers, various error functions).</p><h3>Aside validation</h3><p>To boost my confidence they&#x27;re doing the same thing, I plotted the learning rate over time of both with a large batch size.</p><p><img src="/images/tf_keras_lr.png" alt="Learning rates" parentName="p"/></p><p>The initially faster learning for TensorFlow was consistent, but they end up in exactly the same place every time. They also scored very similarly on the evaluation function (both above 99% on several runs).</p><h2>Speed</h2><p>While I still maintain this is secondary, it&#x27;s easy and interesting to do a benchmark. This isn&#x27;t a great benchmark, as the network is so simple, but I&#x27;m curious to have an indication. So, the time it takes each to train each using ipythons %timeit:</p><ul><li parentName="ul">TensorFlow: 1.42s</li><li parentName="ul">Keras: 1.45s</li></ul><p>Pretty neck and neck. I ran it a few times and the results varied a lot, but I don&#x27;t think there&#x27;s anything in it. However, <a href="https://github.com/wagamamaz/tensorflow-wrapper-compare" parentName="p">this</a> found TensorFlow much (2x) faster and looks like a more real world test - I&#x27;d be slightly inclined to trust that more. But 2x isn&#x27;t <em parentName="p">too</em> bad for me.</p><h2>Conclusion</h2><p>I&#x27;ll start learning Keras properly - the API was way faster and easier to develop with for this test case. Check out the code in <a href="https://github.com/dgmp88/nn-playground/blob/master/frameworks/TensorFlow%20vs%20Keras.ipynb" parentName="p">notebook</a>, looking at both all the way through makes you appreciate the simplicity even more.</p><p>Comment below if you have any thoughts, questions, or strong opinions on DL frameworks!</p></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{}},"page":"/blog/tensorflow-keras","query":{},"buildId":"ExTSNI7gSL9kUh4Eg_T7t","nextExport":true,"autoExport":true,"isFallback":false,"scriptLoader":[]}</script></body></html>